{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Neccessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import emoji\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_train = 'Train\\eRisk2023-T3_Subject'\n",
    "no_of_training_instances = 46\n",
    "data_path_test = 'Test\\eRisk2022-T3_Subject'\n",
    "no_of_testing_instances = 28\n",
    "number_of_questions = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Making the name of the Columns Same**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_coloumns_same(data_path, no_of_instances):\n",
    "\n",
    "    for i in range(1, no_of_instances+1):\n",
    "        df = pd.read_csv(f'{data_path}{i}.csv')\n",
    "        \n",
    "        # If a column is called \"WRITING.TEXT\", rename it to \"WRITING.TEXT.Element:Text\"\n",
    "        if \"WRITING.TEXT\" in df.columns:\n",
    "            df.rename(columns={\"WRITING.TEXT\": \"WRITING.TEXT.Element:Text\"}, inplace=True)\n",
    "            df.to_csv(f'{data_path}{i}.csv', index=False)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Following Data Cleaning operations are performed:**\n",
    "\n",
    "\n",
    "    1. Remove the trailing b\" and the ending \"\n",
    "    2. Replacing unicode characters with their actual characters\n",
    "    3. Replacing emojis with their text descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data_path, no_of_instances):\n",
    "\n",
    "    for i in range(1, no_of_instances + 1):\n",
    "\n",
    "        df = pd.read_csv(f'{data_path}{i}.csv')\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "\n",
    "            text_value = row['WRITING.TEXT.Element:Text']\n",
    "\n",
    "            if isinstance(text_value, str):\n",
    "\n",
    "                if ((text_value[2] == \"'\" or text_value[2] == '\"') and (text_value[-2] == \"'\" or text_value[-2] == '\"') and (text_value[0] == ' ' and text_value[1] == 'b' and text_value[-1] == ' ')):\n",
    "                    df.at[index, 'WRITING.TEXT.Element:Text'] = text_value[3:-2]\n",
    "                    text_value = text_value[3:-2]\n",
    "\n",
    "                # Replace specific substrings\n",
    "                text_value = text_value.replace('\\\\xe2\\\\x80\\\\x99', \"'\")\n",
    "                text_value = text_value.replace('\\\\xe2\\\\x80\\\\x94', \"-\")\n",
    "                text_value = text_value.replace('\\\\n', \" \")\n",
    "\n",
    "                # Define the regular expression pattern to match words starting with \\xf0\\x9f\n",
    "                pattern = r'\\\\xf0\\\\x9f\\S*'\n",
    "                # Replace matched words with a single space\n",
    "                text_value = re.sub(pattern, ' ', text_value)\n",
    "\n",
    "                # Replace emoji with their text description\n",
    "                text_value = emoji.demojize(text_value)\n",
    "                df.at[index, 'WRITING.TEXT.Element:Text'] = text_value\n",
    "                \n",
    "            elif isinstance(text_value, float) and np.isnan(text_value):\n",
    "                # Handle NaN values if needed\n",
    "                pass\n",
    "        \n",
    "        df.to_csv(f'{data_path}{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Making the final DataFrame before applying models to it**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(data_path, no_of_instances, type):\n",
    "    \n",
    "    data_list = []\n",
    "    df_final = pd.DataFrame(columns=['Subject', 'text'])\n",
    "\n",
    "    if type == 'train':\n",
    "        subject_name = 'eRisk2023-T3_Subject'\n",
    "    \n",
    "    else:\n",
    "        subject_name = 'eRisk2022-T3_Subject'\n",
    "\n",
    "    for i in range(1, no_of_instances+1):\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(f'{data_path}{i}.csv')\n",
    "        \n",
    "        # Append the data to df\n",
    "        data_to_append = \"\"\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            if isinstance(row['WRITING.TEXT.Element:Text'], str):\n",
    "                data_to_append = data_to_append + row['WRITING.TEXT.Element:Text'] + \" \"\n",
    "\n",
    "        final_data = {'Subject': f'{subject_name}{i}', 'text': data_to_append}\n",
    "        data_list.append(final_data)\n",
    "\n",
    "    df_final = pd.DataFrame(data_list)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lowercasing\n",
    "2. Removing URLs\n",
    "3. Removing Punctuations\n",
    "4. Removing Stopwords\n",
    "5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text_value = row['text']\n",
    "\n",
    "        # lowercasing\n",
    "        text_value = text_value.lower()\n",
    "\n",
    "        # Remove URLs\n",
    "        text_value = remove_urls(text_value)\n",
    "\n",
    "        # Remove punctuation\n",
    "        text_value = re.sub(r'[^\\w\\s]', '', text_value)\n",
    "\n",
    "        # remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text_value = ' '.join([word for word in text_value.split() if word not in stop_words])\n",
    "\n",
    "        # lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text_value = ' '.join([lemmatizer.lemmatize(word) for word in text_value.split()])\n",
    "        \n",
    "        # store the cleaned text\n",
    "        df.at[index, 'text'] = text_value\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing all the labels in one Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(type):\n",
    "\n",
    "    if type == 'train':\n",
    "\n",
    "        label_list = []\n",
    "        \n",
    "        with open('golden-truth_eRisk2023_T3.txt', 'r') as file:\n",
    "            for line in file:\n",
    "                # go till the first space or tab is found\n",
    "                index = 0\n",
    "                while line[index] != ' ' and line[index] != '\\t':\n",
    "                    index += 1\n",
    "\n",
    "                temp_list = []\n",
    "                total_questions = 0\n",
    "\n",
    "                # there are 22 questions, and each have an integer value separated by a space. store all these values sequentially in temp_list\n",
    "                for i in range(22):\n",
    "                    index += 1\n",
    "                    while (line[index] == ' ' or line[index] == '\\t'):\n",
    "                        index += 1\n",
    "                    temp_list.append(int(line[index]))\n",
    "                    total_questions += 1\n",
    "\n",
    "                label_list.append(temp_list)\n",
    "\n",
    "    else:\n",
    "\n",
    "        label_list = []\n",
    "        \n",
    "        with open('ground-truth_eRisk2022_T3.txt', 'r') as file:\n",
    "            for line in file:\n",
    "                # go till the first space or tab is found\n",
    "                index = 0\n",
    "                while line[index] != ' ' and line[index] != '\\t':\n",
    "                    index += 1\n",
    "\n",
    "                temp_list = []\n",
    "                total_questions = 0\n",
    "\n",
    "                # there are 22 questions, and each have an integer value separated by a space. store all these values sequentially in temp_list\n",
    "                for i in range(22):\n",
    "                    index += 1\n",
    "                    while (line[index] == ' ' or line[index] == '\\t'):\n",
    "                        index += 1\n",
    "                    temp_list.append(int(line[index]))\n",
    "                    total_questions += 1\n",
    "\n",
    "                label_list.append(temp_list)\n",
    "\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = extract_labels('train')\n",
    "labels_test = extract_labels('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_zero_one_error(y_true, y_pred):\n",
    "\n",
    "    individual_errors = []\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        no_of_misclassifications = 0\n",
    "        \n",
    "        for j in range(len(y_true[i])):\n",
    "            if y_true[i][j] != y_pred[i][j]:\n",
    "                no_of_misclassifications += 1\n",
    "\n",
    "        individual_errors.append(no_of_misclassifications/22.0)\n",
    "\n",
    "    return np.mean(individual_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "\n",
    "    individual_errors = []\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        total_error = 0\n",
    "        \n",
    "        for j in range(len(y_true[i])):\n",
    "            total_error += abs(y_true[i][j] - y_pred[i][j])\n",
    "            \n",
    "        individual_errors.append(total_error/22.0)\n",
    "\n",
    "    return np.mean(individual_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macroaveraged_mean_absolute_error(y_true, y_pred):\n",
    "\n",
    "    individual_errors = []\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        total_error = [0, 0, 0, 0, 0, 0, 0]\n",
    "        total_points = [0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "        for j in range(len(y_true[i])):\n",
    "            total_error[y_true[i][j]] += abs(y_true[i][j] - y_pred[i][j])\n",
    "            total_points[y_true[i][j]] += 1\n",
    "\n",
    "        final_error = 0\n",
    "\n",
    "        for k in range(7):\n",
    "            if total_points[k] != 0:\n",
    "                final_error += total_error[k]/(total_points[k]*1.0)\n",
    "        \n",
    "        individual_errors.append(final_error/7.0)\n",
    "\n",
    "    return np.mean(individual_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrained_subscale(y_true, y_pred):\n",
    "    \n",
    "    indices = [0,1,2,3,4]\n",
    "    total_error = 0\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "\n",
    "        system_score = 0\n",
    "        real_score = 0\n",
    "\n",
    "        for j in indices:\n",
    "            system_score += y_pred[i][j]\n",
    "            real_score += y_true[i][j]\n",
    "        \n",
    "        system_score = system_score/(len(indices)*1.0)\n",
    "        real_score = real_score/(len(indices)*1.0)\n",
    "\n",
    "        total_error += (system_score - real_score)**2\n",
    "\n",
    "    total_error = total_error/(len(y_true)*1.0)\n",
    "    total_error = np.sqrt(total_error)\n",
    "\n",
    "    return total_error\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eating_concern_subscale(y_true, y_pred):\n",
    "    \n",
    "    indices = [6,8,12,13,14]\n",
    "    total_error = 0\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "\n",
    "        system_score = 0\n",
    "        real_score = 0\n",
    "\n",
    "        for j in indices:\n",
    "            system_score += y_pred[i][j]\n",
    "            real_score += y_true[i][j]\n",
    "        \n",
    "        system_score = system_score/(len(indices)*1.0)\n",
    "        real_score = real_score/(len(indices)*1.0)\n",
    "\n",
    "        total_error += (system_score - real_score)**2\n",
    "\n",
    "    total_error = total_error/(len(y_true)*1.0)\n",
    "    total_error = np.sqrt(total_error)\n",
    "\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_concern_subscale(y_true, y_pred):\n",
    "    \n",
    "    indices = [5,7,9,10,16,19,20,21]\n",
    "    total_error = 0\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "\n",
    "        system_score = 0\n",
    "        real_score = 0\n",
    "\n",
    "        for j in indices:\n",
    "            system_score += y_pred[i][j]\n",
    "            real_score += y_true[i][j]\n",
    "        \n",
    "        system_score = system_score/(len(indices)*1.0)\n",
    "        real_score = real_score/(len(indices)*1.0)\n",
    "\n",
    "        total_error += (system_score - real_score)**2\n",
    "\n",
    "    total_error = total_error/(len(y_true)*1.0)\n",
    "    total_error = np.sqrt(total_error)\n",
    "\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_concern_subscale(y_true, y_pred):\n",
    "\n",
    "    indices = [7,11,15,17,18]\n",
    "    total_error = 0\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "\n",
    "        system_score = 0\n",
    "        real_score = 0\n",
    "\n",
    "        for j in indices:\n",
    "            system_score += y_pred[i][j]\n",
    "            real_score += y_true[i][j]\n",
    "        \n",
    "        system_score = system_score/(len(indices)*1.0)\n",
    "        real_score = real_score/(len(indices)*1.0)\n",
    "\n",
    "        total_error += (system_score - real_score)**2\n",
    "\n",
    "    total_error = total_error/(len(y_true)*1.0)\n",
    "    total_error = np.sqrt(total_error)\n",
    "\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_ED(y_true, y_pred):\n",
    "\n",
    "    indices_r = [0,1,2,3,4]\n",
    "    indices_e = [6,8,12,13,14]\n",
    "    indices_s = [5,7,9,10,16,19,20,21]\n",
    "    indices_w = [7,11,15,17,18]\n",
    "\n",
    "    size_lists = [5,5,8,5]\n",
    "\n",
    "    total_error = 0\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "\n",
    "        system_score = [0, 0, 0, 0]\n",
    "        real_score = [0, 0, 0, 0]\n",
    "        total_error_system = 0\n",
    "        total_error_real = 0\n",
    "\n",
    "        for j in indices_r:\n",
    "            system_score[0] += y_pred[i][j]\n",
    "            real_score[0] += y_true[i][j]\n",
    "        \n",
    "        for j in indices_e:\n",
    "            system_score[1] += y_pred[i][j]\n",
    "            real_score[1] += y_true[i][j]\n",
    "        \n",
    "        for j in indices_s:\n",
    "            system_score[2] += y_pred[i][j]\n",
    "            real_score[2] += y_true[i][j]\n",
    "        \n",
    "        for j in indices_w:\n",
    "            system_score[3] += y_pred[i][j]\n",
    "            real_score[3] += y_true[i][j]\n",
    "\n",
    "        for k in range(4):\n",
    "            system_score[k] = system_score[k]/(size_lists[k]*1.0)\n",
    "            real_score[k] = real_score[k]/(size_lists[k]*1.0)\n",
    "\n",
    "            total_error_system += system_score[k]\n",
    "            total_error_real += real_score[k]\n",
    "        \n",
    "        total_error_system = total_error_system/(4.0)\n",
    "        total_error_real = total_error_real/(4.0)\n",
    "\n",
    "        total_error += (total_error_system - total_error_real)**2\n",
    "\n",
    "    total_error = total_error/(len(y_true)*1.0)\n",
    "    total_error = np.sqrt(total_error)\n",
    "\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_metrics(y_true, y_pred):\n",
    "\n",
    "    print(\"Mean Zero-One Error: \", mean_zero_one_error(y_true, y_pred))\n",
    "    print(\"Mean Absolute Error: \", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Macroaveraged Mean Absolute Error: \", macroaveraged_mean_absolute_error(y_true, y_pred))\n",
    "    print(\"Restrained Subscale: \", restrained_subscale(y_true, y_pred))\n",
    "    print(\"Eating Concern Subscale: \", eating_concern_subscale(y_true, y_pred))\n",
    "    print(\"Shape Concern Subscale: \", shape_concern_subscale(y_true, y_pred))\n",
    "    print(\"Weight Concern Subscale: \", weight_concern_subscale(y_true, y_pred))\n",
    "    print(\"Global ED: \", global_ED(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def multinomial_naves_bayes(question_number, labels_train, labels_test, df_train, df_test):\n",
    "\n",
    "    nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "    \n",
    "    # Train the model\n",
    "    X_train = df_train['text']\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(len(labels_train)):\n",
    "        y_train.append(labels_train[i][question_number])\n",
    "\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    # Test the model\n",
    "    X_test = df_test['text']\n",
    "    y_pred = nb.predict(X_test)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def linear_SVM(question_number, labels_train, labels_test, df_train, df_test):\n",
    "\n",
    "    sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "\n",
    "    # Train the model\n",
    "    X_train = df_train['text']\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(len(labels_train)):\n",
    "        y_train.append(labels_train[i][question_number])\n",
    "    \n",
    "    sgd.fit(X_train, y_train)\n",
    "\n",
    "    # Test the model\n",
    "    X_test = df_test['text']\n",
    "    y_pred = sgd.predict(X_test)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_regression(question_number, labels_train, labels_test, df_train, df_test):\n",
    "\n",
    "    logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "\n",
    "    # Train the model\n",
    "    X_train = df_train['text']\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(len(labels_train)):\n",
    "        y_train.append(labels_train[i][question_number])\n",
    "    \n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    # Test the model\n",
    "    X_test = df_test['text']\n",
    "    y_pred = logreg.predict(X_test)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eRisk2023-T3_Subject1</td>\n",
       "      <td>idk mate since 1796 using vaccine still waitin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eRisk2023-T3_Subject2</td>\n",
       "      <td>loved ideaconcept feel like film left many thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eRisk2023-T3_Subject3</td>\n",
       "      <td>dont need certification start working h motor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eRisk2023-T3_Subject4</td>\n",
       "      <td>leangains method rept fitness workout tracker ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eRisk2023-T3_Subject5</td>\n",
       "      <td>think opinion system led anxiety havent change...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Subject                                               text\n",
       "0  eRisk2023-T3_Subject1  idk mate since 1796 using vaccine still waitin...\n",
       "1  eRisk2023-T3_Subject2  loved ideaconcept feel like film left many thi...\n",
       "2  eRisk2023-T3_Subject3  dont need certification start working h motor ...\n",
       "3  eRisk2023-T3_Subject4  leangains method rept fitness workout tracker ...\n",
       "4  eRisk2023-T3_Subject5  think opinion system led anxiety havent change..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_coloumns_same(data_path_train, no_of_training_instances)\n",
    "data_cleaning(data_path_train, no_of_training_instances)\n",
    "df_train = make_dataframe(data_path_train, no_of_training_instances, 'train')\n",
    "df_train = data_preprocessing(df_train)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eRisk2023-T3_Subject1</td>\n",
       "      <td>idk mate since 1796 using vaccine still waitin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eRisk2023-T3_Subject2</td>\n",
       "      <td>loved ideaconcept feel like film left many thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eRisk2023-T3_Subject3</td>\n",
       "      <td>dont need certification start working h motor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eRisk2023-T3_Subject4</td>\n",
       "      <td>leangains method rept fitness workout tracker ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eRisk2023-T3_Subject5</td>\n",
       "      <td>think opinion system led anxiety havent change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eRisk2023-T3_Subject6</td>\n",
       "      <td>volunteer waited late 30 family took much ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eRisk2023-T3_Subject7</td>\n",
       "      <td>find self thinking food lot wanting eat etc ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eRisk2023-T3_Subject8</td>\n",
       "      <td>wale country catalonia 3xe2x98xbaxefxb8x8f def...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>eRisk2023-T3_Subject9</td>\n",
       "      <td>function word everything task content word muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>eRisk2023-T3_Subject10</td>\n",
       "      <td>em polypharmacy much 80yo gramma taking idea s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>eRisk2023-T3_Subject11</td>\n",
       "      <td>dont execute enemy kill didnt like hk first fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>eRisk2023-T3_Subject12</td>\n",
       "      <td>id say soap isnt best description taste like t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>eRisk2023-T3_Subject13</td>\n",
       "      <td>done done heart emoticon go conquer ca creativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>eRisk2023-T3_Subject14</td>\n",
       "      <td>yeah mainly eat evening night dont eat bed get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>eRisk2023-T3_Subject15</td>\n",
       "      <td>agreed lol latter ill manifest mermaid orgy fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eRisk2023-T3_Subject16</td>\n",
       "      <td>looking one ga ticket message youre trying get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>eRisk2023-T3_Subject17</td>\n",
       "      <td>dont experience gender envy actually see gende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>eRisk2023-T3_Subject18</td>\n",
       "      <td>thank thats nice say wow thanks awww thank muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>eRisk2023-T3_Subject19</td>\n",
       "      <td>read comment section lot comment posted autist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>eRisk2023-T3_Subject20</td>\n",
       "      <td>also likely relative currency valuation import...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>eRisk2023-T3_Subject21</td>\n",
       "      <td>might bit track post someone explain see much ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>eRisk2023-T3_Subject22</td>\n",
       "      <td>technically yes much im sure would make much d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>eRisk2023-T3_Subject23</td>\n",
       "      <td>geoguessrcom case guy win second racist three ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>eRisk2023-T3_Subject24</td>\n",
       "      <td>use alcohol learnt somewhere alcohol would enh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>eRisk2023-T3_Subject25</td>\n",
       "      <td>kindly rest first world rest first world gun p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>eRisk2023-T3_Subject26</td>\n",
       "      <td>anti matter negative mass negative energy anti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>eRisk2023-T3_Subject27</td>\n",
       "      <td>think thing would fitting prequel gertrude ger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>eRisk2023-T3_Subject28</td>\n",
       "      <td>get never lost weight always feeling empty sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>eRisk2023-T3_Subject29</td>\n",
       "      <td>think personal experience anxiety caused lot c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>eRisk2023-T3_Subject30</td>\n",
       "      <td>jessefestus love new absolute favourite fragra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>eRisk2023-T3_Subject31</td>\n",
       "      <td>yeah ok thanks still dark give one worst as va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>eRisk2023-T3_Subject32</td>\n",
       "      <td>listen feel however want random post russian f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>eRisk2023-T3_Subject33</td>\n",
       "      <td>cramming tooo add study gc cram together count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>eRisk2023-T3_Subject34</td>\n",
       "      <td>beautiful hair maam actually impressed beauty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>eRisk2023-T3_Subject35</td>\n",
       "      <td>think he curious youre phone show bit trust do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>eRisk2023-T3_Subject36</td>\n",
       "      <td>dont get caption trying say power move ixcaxb5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>eRisk2023-T3_Subject37</td>\n",
       "      <td>moldy geode hehe great mind think alike go fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>eRisk2023-T3_Subject38</td>\n",
       "      <td>cool looking game far ill sure join community ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>eRisk2023-T3_Subject39</td>\n",
       "      <td>hey thanks ur answer resource thats really hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>eRisk2023-T3_Subject40</td>\n",
       "      <td>smile love people smile relief lot pressure pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>eRisk2023-T3_Subject41</td>\n",
       "      <td>id recommend talking parent dont care father s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>eRisk2023-T3_Subject42</td>\n",
       "      <td>joke he commercialised huge number product man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>eRisk2023-T3_Subject43</td>\n",
       "      <td>also infertile nothing jealous like understand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>eRisk2023-T3_Subject44</td>\n",
       "      <td>idea siege ok overwatch suck go play tf2 inste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>eRisk2023-T3_Subject45</td>\n",
       "      <td>salsa pickle enchilada popsicle hummus chalupa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>eRisk2023-T3_Subject46</td>\n",
       "      <td>one mainly come im social setting ive tried lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Subject                                               text\n",
       "0    eRisk2023-T3_Subject1  idk mate since 1796 using vaccine still waitin...\n",
       "1    eRisk2023-T3_Subject2  loved ideaconcept feel like film left many thi...\n",
       "2    eRisk2023-T3_Subject3  dont need certification start working h motor ...\n",
       "3    eRisk2023-T3_Subject4  leangains method rept fitness workout tracker ...\n",
       "4    eRisk2023-T3_Subject5  think opinion system led anxiety havent change...\n",
       "5    eRisk2023-T3_Subject6  volunteer waited late 30 family took much ther...\n",
       "6    eRisk2023-T3_Subject7  find self thinking food lot wanting eat etc ac...\n",
       "7    eRisk2023-T3_Subject8  wale country catalonia 3xe2x98xbaxefxb8x8f def...\n",
       "8    eRisk2023-T3_Subject9  function word everything task content word muc...\n",
       "9   eRisk2023-T3_Subject10  em polypharmacy much 80yo gramma taking idea s...\n",
       "10  eRisk2023-T3_Subject11  dont execute enemy kill didnt like hk first fe...\n",
       "11  eRisk2023-T3_Subject12  id say soap isnt best description taste like t...\n",
       "12  eRisk2023-T3_Subject13  done done heart emoticon go conquer ca creativ...\n",
       "13  eRisk2023-T3_Subject14  yeah mainly eat evening night dont eat bed get...\n",
       "14  eRisk2023-T3_Subject15  agreed lol latter ill manifest mermaid orgy fr...\n",
       "15  eRisk2023-T3_Subject16  looking one ga ticket message youre trying get...\n",
       "16  eRisk2023-T3_Subject17  dont experience gender envy actually see gende...\n",
       "17  eRisk2023-T3_Subject18  thank thats nice say wow thanks awww thank muc...\n",
       "18  eRisk2023-T3_Subject19  read comment section lot comment posted autist...\n",
       "19  eRisk2023-T3_Subject20  also likely relative currency valuation import...\n",
       "20  eRisk2023-T3_Subject21  might bit track post someone explain see much ...\n",
       "21  eRisk2023-T3_Subject22  technically yes much im sure would make much d...\n",
       "22  eRisk2023-T3_Subject23  geoguessrcom case guy win second racist three ...\n",
       "23  eRisk2023-T3_Subject24  use alcohol learnt somewhere alcohol would enh...\n",
       "24  eRisk2023-T3_Subject25  kindly rest first world rest first world gun p...\n",
       "25  eRisk2023-T3_Subject26  anti matter negative mass negative energy anti...\n",
       "26  eRisk2023-T3_Subject27  think thing would fitting prequel gertrude ger...\n",
       "27  eRisk2023-T3_Subject28  get never lost weight always feeling empty sto...\n",
       "28  eRisk2023-T3_Subject29  think personal experience anxiety caused lot c...\n",
       "29  eRisk2023-T3_Subject30  jessefestus love new absolute favourite fragra...\n",
       "30  eRisk2023-T3_Subject31  yeah ok thanks still dark give one worst as va...\n",
       "31  eRisk2023-T3_Subject32  listen feel however want random post russian f...\n",
       "32  eRisk2023-T3_Subject33  cramming tooo add study gc cram together count...\n",
       "33  eRisk2023-T3_Subject34  beautiful hair maam actually impressed beauty ...\n",
       "34  eRisk2023-T3_Subject35  think he curious youre phone show bit trust do...\n",
       "35  eRisk2023-T3_Subject36  dont get caption trying say power move ixcaxb5...\n",
       "36  eRisk2023-T3_Subject37  moldy geode hehe great mind think alike go fin...\n",
       "37  eRisk2023-T3_Subject38  cool looking game far ill sure join community ...\n",
       "38  eRisk2023-T3_Subject39  hey thanks ur answer resource thats really hel...\n",
       "39  eRisk2023-T3_Subject40  smile love people smile relief lot pressure pe...\n",
       "40  eRisk2023-T3_Subject41  id recommend talking parent dont care father s...\n",
       "41  eRisk2023-T3_Subject42  joke he commercialised huge number product man...\n",
       "42  eRisk2023-T3_Subject43  also infertile nothing jealous like understand...\n",
       "43  eRisk2023-T3_Subject44  idea siege ok overwatch suck go play tf2 inste...\n",
       "44  eRisk2023-T3_Subject45  salsa pickle enchilada popsicle hummus chalupa...\n",
       "45  eRisk2023-T3_Subject46  one mainly come im social setting ive tried lo..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eRisk2022-T3_Subject1</td>\n",
       "      <td>thats thats post fucking utterly completely lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eRisk2022-T3_Subject2</td>\n",
       "      <td>shes wrong 25 warm rtightpussy thats spicy gir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eRisk2022-T3_Subject3</td>\n",
       "      <td>feel way ill sometimes starve im feeling stres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eRisk2022-T3_Subject4</td>\n",
       "      <td>yoga vvvv helpful slowly concentrate movement ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eRisk2022-T3_Subject5</td>\n",
       "      <td>im experience ive owned patent leather suede b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Subject                                               text\n",
       "0  eRisk2022-T3_Subject1  thats thats post fucking utterly completely lo...\n",
       "1  eRisk2022-T3_Subject2  shes wrong 25 warm rtightpussy thats spicy gir...\n",
       "2  eRisk2022-T3_Subject3  feel way ill sometimes starve im feeling stres...\n",
       "3  eRisk2022-T3_Subject4  yoga vvvv helpful slowly concentrate movement ...\n",
       "4  eRisk2022-T3_Subject5  im experience ive owned patent leather suede b..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_coloumns_same(data_path_test, no_of_testing_instances)\n",
    "data_cleaning(data_path_test, no_of_testing_instances)\n",
    "df_test = make_dataframe(data_path_test, no_of_testing_instances, 'test')\n",
    "df_test = data_preprocessing(df_test)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eRisk2022-T3_Subject1</td>\n",
       "      <td>thats thats post fucking utterly completely lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eRisk2022-T3_Subject2</td>\n",
       "      <td>shes wrong 25 warm rtightpussy thats spicy gir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eRisk2022-T3_Subject3</td>\n",
       "      <td>feel way ill sometimes starve im feeling stres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eRisk2022-T3_Subject4</td>\n",
       "      <td>yoga vvvv helpful slowly concentrate movement ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eRisk2022-T3_Subject5</td>\n",
       "      <td>im experience ive owned patent leather suede b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eRisk2022-T3_Subject6</td>\n",
       "      <td>one find kidnapping inpregnation thing unsette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eRisk2022-T3_Subject7</td>\n",
       "      <td>home rholup 175 favorite place visit paris wen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eRisk2022-T3_Subject8</td>\n",
       "      <td>youre friend could honest directly tell making...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>eRisk2022-T3_Subject9</td>\n",
       "      <td>oh fun ive lost equivalent beagle 16kg yup gnu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>eRisk2022-T3_Subject10</td>\n",
       "      <td>wash hair often greasy hair absorbs scent much...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>eRisk2022-T3_Subject11</td>\n",
       "      <td>linkedin infojobs indeed quite bunch job offer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>eRisk2022-T3_Subject12</td>\n",
       "      <td>something someone used fun one day almost slee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>eRisk2022-T3_Subject13</td>\n",
       "      <td>alright guess really good thought something wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>eRisk2022-T3_Subject14</td>\n",
       "      <td>buying curved barbell measurement bar distance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>eRisk2022-T3_Subject15</td>\n",
       "      <td>ive diagnosed add last june year like 6 year d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eRisk2022-T3_Subject16</td>\n",
       "      <td>never get everything usually use hour couple h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>eRisk2022-T3_Subject17</td>\n",
       "      <td>followed story tiktok first got bobby sadly ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>eRisk2022-T3_Subject18</td>\n",
       "      <td>see thanks video really nice onboarding screen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>eRisk2022-T3_Subject19</td>\n",
       "      <td>im student full time work 20 25h week work wee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>eRisk2022-T3_Subject20</td>\n",
       "      <td>11 hi thought vegeta goku 5 year apart vegeta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>eRisk2022-T3_Subject21</td>\n",
       "      <td>fake calorie 8am calorie 8pm love cheese calci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>eRisk2022-T3_Subject22</td>\n",
       "      <td>prozac fda approved bulimia could really help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>eRisk2022-T3_Subject23</td>\n",
       "      <td>tf get invited famous party im sorry guy secon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>eRisk2022-T3_Subject24</td>\n",
       "      <td>hard pick favorite animal ω im america selecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>eRisk2022-T3_Subject25</td>\n",
       "      <td>disqualified limbo contest yolkswagen pair kni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>eRisk2022-T3_Subject26</td>\n",
       "      <td>cute hehe emergency call cop make sure always ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>eRisk2022-T3_Subject27</td>\n",
       "      <td>theyre jewish chance dont know know someone kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>eRisk2022-T3_Subject28</td>\n",
       "      <td>part 3 task 4 4 redo go 5 hour timer second ti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Subject                                               text\n",
       "0    eRisk2022-T3_Subject1  thats thats post fucking utterly completely lo...\n",
       "1    eRisk2022-T3_Subject2  shes wrong 25 warm rtightpussy thats spicy gir...\n",
       "2    eRisk2022-T3_Subject3  feel way ill sometimes starve im feeling stres...\n",
       "3    eRisk2022-T3_Subject4  yoga vvvv helpful slowly concentrate movement ...\n",
       "4    eRisk2022-T3_Subject5  im experience ive owned patent leather suede b...\n",
       "5    eRisk2022-T3_Subject6  one find kidnapping inpregnation thing unsette...\n",
       "6    eRisk2022-T3_Subject7  home rholup 175 favorite place visit paris wen...\n",
       "7    eRisk2022-T3_Subject8  youre friend could honest directly tell making...\n",
       "8    eRisk2022-T3_Subject9  oh fun ive lost equivalent beagle 16kg yup gnu...\n",
       "9   eRisk2022-T3_Subject10  wash hair often greasy hair absorbs scent much...\n",
       "10  eRisk2022-T3_Subject11  linkedin infojobs indeed quite bunch job offer...\n",
       "11  eRisk2022-T3_Subject12  something someone used fun one day almost slee...\n",
       "12  eRisk2022-T3_Subject13  alright guess really good thought something wr...\n",
       "13  eRisk2022-T3_Subject14  buying curved barbell measurement bar distance...\n",
       "14  eRisk2022-T3_Subject15  ive diagnosed add last june year like 6 year d...\n",
       "15  eRisk2022-T3_Subject16  never get everything usually use hour couple h...\n",
       "16  eRisk2022-T3_Subject17  followed story tiktok first got bobby sadly ca...\n",
       "17  eRisk2022-T3_Subject18  see thanks video really nice onboarding screen...\n",
       "18  eRisk2022-T3_Subject19  im student full time work 20 25h week work wee...\n",
       "19  eRisk2022-T3_Subject20  11 hi thought vegeta goku 5 year apart vegeta ...\n",
       "20  eRisk2022-T3_Subject21  fake calorie 8am calorie 8pm love cheese calci...\n",
       "21  eRisk2022-T3_Subject22  prozac fda approved bulimia could really help ...\n",
       "22  eRisk2022-T3_Subject23  tf get invited famous party im sorry guy secon...\n",
       "23  eRisk2022-T3_Subject24  hard pick favorite animal ω im america selecte...\n",
       "24  eRisk2022-T3_Subject25  disqualified limbo contest yolkswagen pair kni...\n",
       "25  eRisk2022-T3_Subject26  cute hehe emergency call cop make sure always ...\n",
       "26  eRisk2022-T3_Subject27  theyre jewish chance dont know know someone kn...\n",
       "27  eRisk2022-T3_Subject28  part 3 task 4 4 redo go 5 hour timer second ti..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1\n",
      "Question 2\n",
      "Question 3\n",
      "Question 4\n",
      "Question 5\n",
      "Question 6\n",
      "Question 7\n",
      "Question 8\n",
      "Question 9\n",
      "Question 10\n",
      "Question 11\n",
      "Question 12\n",
      "Question 13\n",
      "Question 14\n",
      "Question 15\n",
      "Question 16\n",
      "Question 17\n",
      "Question 18\n",
      "Question 19\n",
      "Question 20\n",
      "Question 21\n",
      "Question 22\n"
     ]
    }
   ],
   "source": [
    "multinomial_naive_bayes_predictions = []\n",
    "\n",
    "for i in range(0, number_of_questions):\n",
    "    print(f'Question {i+1}')\n",
    "    multinomial_naive_bayes_predictions.append(multinomial_naves_bayes(i, labels_train, labels_test, df_train, df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 22)\n"
     ]
    }
   ],
   "source": [
    "multinomial_naive_bayes_predictions_np = np.array(multinomial_naive_bayes_predictions)\n",
    "multinomial_naive_bayes_predictions_np = multinomial_naive_bayes_predictions_np.T\n",
    "print(multinomial_naive_bayes_predictions_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 22)\n"
     ]
    }
   ],
   "source": [
    "labels_test_np = np.array(labels_test)\n",
    "print(labels_test_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Zero-One Error:  0.6753246753246753\n",
      "Mean Absolute Error:  2.340909090909091\n",
      "Macroaveraged Mean Absolute Error:  1.7449807990523618\n",
      "Restrained Subscale:  2.5793133083937563\n",
      "Eating Concern Subscale:  3.0173308924090034\n",
      "Shape Concern Subscale:  1.9142324086394824\n",
      "Weight Concern Subscale:  1.9142324086394824\n",
      "Global ED:  1.8380655410065536\n"
     ]
    }
   ],
   "source": [
    "find_all_metrics(labels_test_np, multinomial_naive_bayes_predictions_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1\n",
      "Question 2\n",
      "Question 3\n",
      "Question 4\n",
      "Question 5\n",
      "Question 6\n",
      "Question 7\n",
      "Question 8\n",
      "Question 9\n",
      "Question 10\n",
      "Question 11\n",
      "Question 12\n",
      "Question 13\n",
      "Question 14\n",
      "Question 15\n",
      "Question 16\n",
      "Question 17\n",
      "Question 18\n",
      "Question 19\n",
      "Question 20\n",
      "Question 21\n",
      "Question 22\n"
     ]
    }
   ],
   "source": [
    "linear_support_vector_machine_predictions = []\n",
    "\n",
    "for i in range(0, number_of_questions):\n",
    "    print(f'Question {i+1}')\n",
    "    linear_support_vector_machine_predictions.append(linear_SVM(i, labels_train, labels_test, df_train, df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 22)\n"
     ]
    }
   ],
   "source": [
    "linear_support_vector_machine_predictions_np = np.array(linear_support_vector_machine_predictions)\n",
    "linear_support_vector_machine_predictions_np = linear_support_vector_machine_predictions_np.T\n",
    "print(linear_support_vector_machine_predictions_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Zero-One Error:  0.7012987012987012\n",
      "Mean Absolute Error:  1.974025974025974\n",
      "Macroaveraged Mean Absolute Error:  1.560444493074058\n",
      "Restrained Subscale:  1.9867417691141587\n",
      "Eating Concern Subscale:  1.6274432182326448\n",
      "Shape Concern Subscale:  1.4177446878757824\n",
      "Weight Concern Subscale:  1.4177446878757824\n",
      "Global ED:  1.360584437350624\n"
     ]
    }
   ],
   "source": [
    "find_all_metrics(labels_test_np, linear_support_vector_machine_predictions_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1\n",
      "Question 2\n",
      "Question 3\n",
      "Question 4\n",
      "Question 5\n",
      "Question 6\n",
      "Question 7\n",
      "Question 8\n",
      "Question 9\n",
      "Question 10\n",
      "Question 11\n",
      "Question 12\n",
      "Question 13\n",
      "Question 14\n",
      "Question 15\n",
      "Question 16\n",
      "Question 17\n",
      "Question 18\n",
      "Question 19\n",
      "Question 20\n",
      "Question 21\n",
      "Question 22\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_predictions = []\n",
    "\n",
    "for i in range(0, number_of_questions):\n",
    "    print(f'Question {i+1}')\n",
    "    logistic_regression_predictions.append(logistic_regression(i, labels_train, labels_test, df_train, df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 22)\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_predictions_np = np.array(logistic_regression_predictions)\n",
    "logistic_regression_predictions_np = logistic_regression_predictions_np.T\n",
    "print(logistic_regression_predictions_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Zero-One Error:  0.6720779220779222\n",
      "Mean Absolute Error:  2.043831168831169\n",
      "Macroaveraged Mean Absolute Error:  1.6271578180144235\n",
      "Restrained Subscale:  2.431636720987973\n",
      "Eating Concern Subscale:  2.1480888515807983\n",
      "Shape Concern Subscale:  1.6234883078464981\n",
      "Weight Concern Subscale:  1.6234883078464981\n",
      "Global ED:  1.5417501686684796\n"
     ]
    }
   ],
   "source": [
    "find_all_metrics(labels_test_np, logistic_regression_predictions_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91900\\AppData\\Local\\Temp\\ipykernel_28904\\1164884251.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_df = summary_df.append({'Model': 'Multinomial Naive Bayes', 'Mean Zero-One Error': mean_zero_one_error(labels_test_np, multinomial_naive_bayes_predictions_np), 'Mean Absolute Error': mean_absolute_error(labels_test_np, multinomial_naive_bayes_predictions_np), 'Macroaveraged Mean Absolute Error': macroaveraged_mean_absolute_error(labels_test_np, multinomial_naive_bayes_predictions_np), 'Restrained Subscale': restrained_subscale(labels_test_np, multinomial_naive_bayes_predictions_np), 'Eating Concern Subscale': eating_concern_subscale(labels_test_np, multinomial_naive_bayes_predictions_np), 'Shape Concern Subscale': shape_concern_subscale(labels_test_np, multinomial_naive_bayes_predictions_np), 'Weight Concern Subscale': weight_concern_subscale(labels_test_np, multinomial_naive_bayes_predictions_np), 'Global ED': global_ED(labels_test_np, multinomial_naive_bayes_predictions_np)}, ignore_index=True)\n",
      "C:\\Users\\91900\\AppData\\Local\\Temp\\ipykernel_28904\\1164884251.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_df = summary_df.append({'Model': 'Linear Support Vector Machine', 'Mean Zero-One Error': mean_zero_one_error(labels_test_np, linear_support_vector_machine_predictions_np), 'Mean Absolute Error': mean_absolute_error(labels_test_np, linear_support_vector_machine_predictions_np), 'Macroaveraged Mean Absolute Error': macroaveraged_mean_absolute_error(labels_test_np, linear_support_vector_machine_predictions_np), 'Restrained Subscale': restrained_subscale(labels_test_np, linear_support_vector_machine_predictions_np), 'Eating Concern Subscale': eating_concern_subscale(labels_test_np, linear_support_vector_machine_predictions_np), 'Shape Concern Subscale': shape_concern_subscale(labels_test_np, linear_support_vector_machine_predictions_np), 'Weight Concern Subscale': weight_concern_subscale(labels_test_np, linear_support_vector_machine_predictions_np), 'Global ED': global_ED(labels_test_np, linear_support_vector_machine_predictions_np)}, ignore_index=True)\n",
      "C:\\Users\\91900\\AppData\\Local\\Temp\\ipykernel_28904\\1164884251.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_df = summary_df.append({'Model': 'Logistic Regression', 'Mean Zero-One Error': mean_zero_one_error(labels_test_np, logistic_regression_predictions_np), 'Mean Absolute Error': mean_absolute_error(labels_test_np, logistic_regression_predictions_np), 'Macroaveraged Mean Absolute Error': macroaveraged_mean_absolute_error(labels_test_np, logistic_regression_predictions_np), 'Restrained Subscale': restrained_subscale(labels_test_np, logistic_regression_predictions_np), 'Eating Concern Subscale': eating_concern_subscale(labels_test_np, logistic_regression_predictions_np), 'Shape Concern Subscale': shape_concern_subscale(labels_test_np, logistic_regression_predictions_np), 'Weight Concern Subscale': weight_concern_subscale(labels_test_np, logistic_regression_predictions_np), 'Global ED': global_ED(labels_test_np, logistic_regression_predictions_np)}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "summary_df = pd.DataFrame(columns=['Model', 'Mean Zero-One Error', 'Mean Absolute Error', 'Macroaveraged Mean Absolute Error', 'Restrained Subscale', 'Eating Concern Subscale', 'Shape Concern Subscale', 'Weight Concern Subscale', 'Global ED'])\n",
    "\n",
    "summary_df = summary_df.append({'Model': 'Multinomial Naive Bayes', 'Mean Zero-One Error': mean_zero_one_error(labels_test_np, multinomial_naive_bayes_predictions_np), 'Mean Absolute Error': mean_absolute_error(labels_test_np, multinomial_naive_bayes_predictions_np), 'Macroaveraged Mean Absolute Error': macroaveraged_mean_absolute_error(labels_test_np, multinomial_naive_bayes_predictions_np), 'Restrained Subscale': restrained_subscale(labels_test_np, multinomial_naive_bayes_predictions_np), 'Eating Concern Subscale': eating_concern_subscale(labels_test_np, multinomial_naive_bayes_predictions_np), 'Shape Concern Subscale': shape_concern_subscale(labels_test_np, multinomial_naive_bayes_predictions_np), 'Weight Concern Subscale': weight_concern_subscale(labels_test_np, multinomial_naive_bayes_predictions_np), 'Global ED': global_ED(labels_test_np, multinomial_naive_bayes_predictions_np)}, ignore_index=True)\n",
    "summary_df = summary_df.append({'Model': 'Linear Support Vector Machine', 'Mean Zero-One Error': mean_zero_one_error(labels_test_np, linear_support_vector_machine_predictions_np), 'Mean Absolute Error': mean_absolute_error(labels_test_np, linear_support_vector_machine_predictions_np), 'Macroaveraged Mean Absolute Error': macroaveraged_mean_absolute_error(labels_test_np, linear_support_vector_machine_predictions_np), 'Restrained Subscale': restrained_subscale(labels_test_np, linear_support_vector_machine_predictions_np), 'Eating Concern Subscale': eating_concern_subscale(labels_test_np, linear_support_vector_machine_predictions_np), 'Shape Concern Subscale': shape_concern_subscale(labels_test_np, linear_support_vector_machine_predictions_np), 'Weight Concern Subscale': weight_concern_subscale(labels_test_np, linear_support_vector_machine_predictions_np), 'Global ED': global_ED(labels_test_np, linear_support_vector_machine_predictions_np)}, ignore_index=True)\n",
    "summary_df = summary_df.append({'Model': 'Logistic Regression', 'Mean Zero-One Error': mean_zero_one_error(labels_test_np, logistic_regression_predictions_np), 'Mean Absolute Error': mean_absolute_error(labels_test_np, logistic_regression_predictions_np), 'Macroaveraged Mean Absolute Error': macroaveraged_mean_absolute_error(labels_test_np, logistic_regression_predictions_np), 'Restrained Subscale': restrained_subscale(labels_test_np, logistic_regression_predictions_np), 'Eating Concern Subscale': eating_concern_subscale(labels_test_np, logistic_regression_predictions_np), 'Shape Concern Subscale': shape_concern_subscale(labels_test_np, logistic_regression_predictions_np), 'Weight Concern Subscale': weight_concern_subscale(labels_test_np, logistic_regression_predictions_np), 'Global ED': global_ED(labels_test_np, logistic_regression_predictions_np)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean Zero-One Error</th>\n",
       "      <th>Mean Absolute Error</th>\n",
       "      <th>Macroaveraged Mean Absolute Error</th>\n",
       "      <th>Restrained Subscale</th>\n",
       "      <th>Eating Concern Subscale</th>\n",
       "      <th>Shape Concern Subscale</th>\n",
       "      <th>Weight Concern Subscale</th>\n",
       "      <th>Global ED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.675325</td>\n",
       "      <td>2.340909</td>\n",
       "      <td>1.744981</td>\n",
       "      <td>2.579313</td>\n",
       "      <td>3.017331</td>\n",
       "      <td>1.914232</td>\n",
       "      <td>1.914232</td>\n",
       "      <td>1.838066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Support Vector Machine</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>1.974026</td>\n",
       "      <td>1.560444</td>\n",
       "      <td>1.986742</td>\n",
       "      <td>1.627443</td>\n",
       "      <td>1.417745</td>\n",
       "      <td>1.417745</td>\n",
       "      <td>1.360584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.672078</td>\n",
       "      <td>2.043831</td>\n",
       "      <td>1.627158</td>\n",
       "      <td>2.431637</td>\n",
       "      <td>2.148089</td>\n",
       "      <td>1.623488</td>\n",
       "      <td>1.623488</td>\n",
       "      <td>1.541750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model  Mean Zero-One Error  Mean Absolute Error  \\\n",
       "0        Multinomial Naive Bayes             0.675325             2.340909   \n",
       "1  Linear Support Vector Machine             0.701299             1.974026   \n",
       "2            Logistic Regression             0.672078             2.043831   \n",
       "\n",
       "   Macroaveraged Mean Absolute Error  Restrained Subscale  \\\n",
       "0                           1.744981             2.579313   \n",
       "1                           1.560444             1.986742   \n",
       "2                           1.627158             2.431637   \n",
       "\n",
       "   Eating Concern Subscale  Shape Concern Subscale  Weight Concern Subscale  \\\n",
       "0                 3.017331                1.914232                 1.914232   \n",
       "1                 1.627443                1.417745                 1.417745   \n",
       "2                 2.148089                1.623488                 1.623488   \n",
       "\n",
       "   Global ED  \n",
       "0   1.838066  \n",
       "1   1.360584  \n",
       "2   1.541750  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
