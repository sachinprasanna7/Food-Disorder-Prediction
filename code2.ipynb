{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import emoji\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset\\eRisk2023-T3_Subject1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>WRITING.TITLE</th>\n",
       "      <th>WRITING.DATE</th>\n",
       "      <th>WRITING.INFO</th>\n",
       "      <th>WRITING.TEXT.Element:Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eRisk2023-T3_Subject1</td>\n",
       "      <td>b' '</td>\n",
       "      <td>b'2022-10-18 16:54:09'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>Idk mate, since 1796 we have been using vaccin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eRisk2023-T3_Subject1</td>\n",
       "      <td>b' '</td>\n",
       "      <td>b'2022-10-18 09:18:57'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>&gt;No disrespect intended I'm sure  How the actu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eRisk2023-T3_Subject1</td>\n",
       "      <td>b' '</td>\n",
       "      <td>b'2022-10-18 09:12:08'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>The older kids would and the baby would see th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eRisk2023-T3_Subject1</td>\n",
       "      <td>b' '</td>\n",
       "      <td>b'2022-10-18 09:11:46'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eRisk2023-T3_Subject1</td>\n",
       "      <td>b' '</td>\n",
       "      <td>b'2022-10-18 09:11:23'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>It's also used when the head is deformed becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>eRisk2023-T3_Subject1</td>\n",
       "      <td>b\"I'm tired and Idk what to do\"</td>\n",
       "      <td>b'2021-10-22 12:00:56'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>This week I have been extremely tired, it does...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>eRisk2023-T3_Subject1</td>\n",
       "      <td>b'My bus stop has a forest behind (the phone ...</td>\n",
       "      <td>b'2021-10-19 19:59:03'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>eRisk2023-T3_Subject1</td>\n",
       "      <td>b\"In a comment section about a girl being rap...</td>\n",
       "      <td>b'2021-10-14 11:09:40'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>eRisk2023-T3_Subject1</td>\n",
       "      <td>b\"What's the command to generate a nether por...</td>\n",
       "      <td>b'2021-10-11 13:02:38'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>I have tried to search the command in websites...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>eRisk2023-T3_Subject1</td>\n",
       "      <td>b\"In a post about a girl being rap*d.... I'm ...</td>\n",
       "      <td>b'2021-10-10 22:59:47'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>925 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ID                                      WRITING.TITLE  \\\n",
       "0    eRisk2023-T3_Subject1                                              b' '    \n",
       "1    eRisk2023-T3_Subject1                                              b' '    \n",
       "2    eRisk2023-T3_Subject1                                              b' '    \n",
       "3    eRisk2023-T3_Subject1                                              b' '    \n",
       "4    eRisk2023-T3_Subject1                                              b' '    \n",
       "..                     ...                                                ...   \n",
       "920  eRisk2023-T3_Subject1                   b\"I'm tired and Idk what to do\"    \n",
       "921  eRisk2023-T3_Subject1   b'My bus stop has a forest behind (the phone ...   \n",
       "922  eRisk2023-T3_Subject1   b\"In a comment section about a girl being rap...   \n",
       "923  eRisk2023-T3_Subject1   b\"What's the command to generate a nether por...   \n",
       "924  eRisk2023-T3_Subject1   b\"In a post about a girl being rap*d.... I'm ...   \n",
       "\n",
       "                 WRITING.DATE   WRITING.INFO  \\\n",
       "0     b'2022-10-18 16:54:09'    reddit post    \n",
       "1     b'2022-10-18 09:18:57'    reddit post    \n",
       "2     b'2022-10-18 09:12:08'    reddit post    \n",
       "3     b'2022-10-18 09:11:46'    reddit post    \n",
       "4     b'2022-10-18 09:11:23'    reddit post    \n",
       "..                        ...            ...   \n",
       "920   b'2021-10-22 12:00:56'    reddit post    \n",
       "921   b'2021-10-19 19:59:03'    reddit post    \n",
       "922   b'2021-10-14 11:09:40'    reddit post    \n",
       "923   b'2021-10-11 13:02:38'    reddit post    \n",
       "924   b'2021-10-10 22:59:47'    reddit post    \n",
       "\n",
       "                             WRITING.TEXT.Element:Text  \n",
       "0    Idk mate, since 1796 we have been using vaccin...  \n",
       "1    >No disrespect intended I'm sure  How the actu...  \n",
       "2    The older kids would and the baby would see th...  \n",
       "3                                                   No  \n",
       "4    It's also used when the head is deformed becau...  \n",
       "..                                                 ...  \n",
       "920  This week I have been extremely tired, it does...  \n",
       "921                                                NaN  \n",
       "922                                                NaN  \n",
       "923  I have tried to search the command in websites...  \n",
       "924                                                NaN  \n",
       "\n",
       "[925 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name uniform check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'WRITING.TEXT' column found in subject_1.csv\n",
      "No 'WRITING.TEXT' column found in subject_2.csv\n",
      "No 'WRITING.TEXT' column found in subject_3.csv\n",
      "No 'WRITING.TEXT' column found in subject_4.csv\n",
      "No 'WRITING.TEXT' column found in subject_5.csv\n",
      "No 'WRITING.TEXT' column found in subject_6.csv\n",
      "No 'WRITING.TEXT' column found in subject_7.csv\n",
      "No 'WRITING.TEXT' column found in subject_8.csv\n",
      "No 'WRITING.TEXT' column found in subject_9.csv\n",
      "No 'WRITING.TEXT' column found in subject_10.csv\n",
      "No 'WRITING.TEXT' column found in subject_11.csv\n",
      "No 'WRITING.TEXT' column found in subject_12.csv\n",
      "No 'WRITING.TEXT' column found in subject_13.csv\n",
      "No 'WRITING.TEXT' column found in subject_14.csv\n",
      "No 'WRITING.TEXT' column found in subject_15.csv\n",
      "No 'WRITING.TEXT' column found in subject_16.csv\n",
      "No 'WRITING.TEXT' column found in subject_17.csv\n",
      "No 'WRITING.TEXT' column found in subject_18.csv\n",
      "No 'WRITING.TEXT' column found in subject_19.csv\n",
      "No 'WRITING.TEXT' column found in subject_20.csv\n",
      "No 'WRITING.TEXT' column found in subject_21.csv\n",
      "No 'WRITING.TEXT' column found in subject_22.csv\n",
      "No 'WRITING.TEXT' column found in subject_23.csv\n",
      "No 'WRITING.TEXT' column found in subject_24.csv\n",
      "No 'WRITING.TEXT' column found in subject_25.csv\n",
      "No 'WRITING.TEXT' column found in subject_26.csv\n",
      "No 'WRITING.TEXT' column found in subject_27.csv\n",
      "No 'WRITING.TEXT' column found in subject_28.csv\n",
      "No 'WRITING.TEXT' column found in subject_29.csv\n",
      "No 'WRITING.TEXT' column found in subject_30.csv\n",
      "No 'WRITING.TEXT' column found in subject_31.csv\n",
      "No 'WRITING.TEXT' column found in subject_32.csv\n",
      "No 'WRITING.TEXT' column found in subject_33.csv\n",
      "No 'WRITING.TEXT' column found in subject_34.csv\n",
      "No 'WRITING.TEXT' column found in subject_35.csv\n",
      "No 'WRITING.TEXT' column found in subject_36.csv\n",
      "No 'WRITING.TEXT' column found in subject_37.csv\n",
      "No 'WRITING.TEXT' column found in subject_38.csv\n",
      "No 'WRITING.TEXT' column found in subject_39.csv\n",
      "No 'WRITING.TEXT' column found in subject_40.csv\n",
      "No 'WRITING.TEXT' column found in subject_41.csv\n",
      "No 'WRITING.TEXT' column found in subject_42.csv\n",
      "No 'WRITING.TEXT' column found in subject_43.csv\n",
      "No 'WRITING.TEXT' column found in subject_44.csv\n",
      "No 'WRITING.TEXT' column found in subject_45.csv\n",
      "No 'WRITING.TEXT' column found in subject_46.csv\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 47):\n",
    "    df = pd.read_csv(f'Dataset\\eRisk2023-T3_Subject{i}.csv')\n",
    "    \n",
    "    # If a column is called \"WRITING.TEXT\", rename it to \"WRITING.TEXT.Element:Text\"\n",
    "    if \"WRITING.TEXT\" in df.columns:\n",
    "        df.rename(columns={\"WRITING.TEXT\": \"WRITING.TEXT.Element:Text\"}, inplace=True)\n",
    "        df.to_csv(f'Dataset\\eRisk2023-T3_Subject{i}.csv', index=False)  # Save the modified DataFrame\n",
    "        print(f\"Renamed 'WRITING.TEXT' to 'WRITING.TEXT.Element:Text' in subject_{i}.csv\")\n",
    "    else:\n",
    "        print(f\"No 'WRITING.TEXT' column found in subject_{i}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 47):\n",
    "\n",
    "    df = pd.read_csv(f'Dataset\\eRisk2023-T3_Subject{i}.csv')\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        text_value = row['WRITING.TEXT.Element:Text']\n",
    "\n",
    "        if isinstance(text_value, str):\n",
    "\n",
    "            if ((text_value[2] == \"'\" or text_value[2] == '\"') and (text_value[-2] == \"'\" or text_value[-2] == '\"') and (text_value[0] == ' ' and text_value[1] == 'b' and text_value[-1] == ' ')):\n",
    "                df.at[index, 'WRITING.TEXT.Element:Text'] = text_value[3:-2]\n",
    "                text_value = text_value[3:-2]\n",
    "\n",
    "            # Replace specific substrings\n",
    "            text_value = text_value.replace('\\\\xe2\\\\x80\\\\x99', \"'\")\n",
    "            text_value = text_value.replace('\\\\xe2\\\\x80\\\\x94', \"-\")\n",
    "            text_value = text_value.replace('\\\\n', \" \")\n",
    "\n",
    "            # Define the regular expression pattern to match words starting with \\xf0\\x9f\n",
    "            pattern = r'\\\\xf0\\\\x9f\\S*'\n",
    "            # Replace matched words with a single space\n",
    "            text_value = re.sub(pattern, ' ', text_value)\n",
    "\n",
    "            # Replace emoji with their text description\n",
    "            text_value = emoji.demojize(text_value)\n",
    "            df.at[index, 'WRITING.TEXT.Element:Text'] = text_value\n",
    "            \n",
    "        elif isinstance(text_value, float) and np.isnan(text_value):\n",
    "            # Handle NaN values if needed\n",
    "            pass\n",
    "\n",
    "    df.to_csv(f'Dataset\\eRisk2023-T3_Subject{i}.csv', index=False)  # Save the modified DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>WRITING.TITLE</th>\n",
       "      <th>WRITING.DATE</th>\n",
       "      <th>WRITING.INFO</th>\n",
       "      <th>WRITING.TEXT.Element:Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eRisk2023-T3_Subject4</td>\n",
       "      <td>b' '</td>\n",
       "      <td>b'2022-09-07 15:28:59'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>For the leangains method: RePT Fitness workout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eRisk2023-T3_Subject4</td>\n",
       "      <td>b' '</td>\n",
       "      <td>b'2022-07-14 23:56:57'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>Is there some kind of nuance that I\\'m missing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eRisk2023-T3_Subject4</td>\n",
       "      <td>b' '</td>\n",
       "      <td>b'2022-06-29 11:06:36'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>If you connect the app to google fit, you can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eRisk2023-T3_Subject4</td>\n",
       "      <td>b' '</td>\n",
       "      <td>b'2022-06-28 18:54:00'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>It's the EufyLife app that works with the Eufy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eRisk2023-T3_Subject4</td>\n",
       "      <td>b' '</td>\n",
       "      <td>b'2022-06-28 18:31:07'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>As long as I've got enough black coffee to get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>eRisk2023-T3_Subject4</td>\n",
       "      <td>b' '</td>\n",
       "      <td>b'2019-09-20 09:20:37'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>Oh, yeah, so it is. [sad emoticon] Yeah, I've ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>eRisk2023-T3_Subject4</td>\n",
       "      <td>b' '</td>\n",
       "      <td>b'2019-09-19 14:53:23'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>Last week a group of people changed all of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>eRisk2023-T3_Subject4</td>\n",
       "      <td>b' '</td>\n",
       "      <td>b'2019-09-03 13:54:33'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>500 calorie lunch 200g of leftover barbecued b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>eRisk2023-T3_Subject4</td>\n",
       "      <td>b'Keeping it simple'</td>\n",
       "      <td>b'2019-09-03 13:50:30'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>eRisk2023-T3_Subject4</td>\n",
       "      <td>b' '</td>\n",
       "      <td>b'2019-08-22 11:17:49'</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>Some people criticise the programme but I thin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ID           WRITING.TITLE              WRITING.DATE  \\\n",
       "0    eRisk2023-T3_Subject4                   b' '    b'2022-09-07 15:28:59'    \n",
       "1    eRisk2023-T3_Subject4                   b' '    b'2022-07-14 23:56:57'    \n",
       "2    eRisk2023-T3_Subject4                   b' '    b'2022-06-29 11:06:36'    \n",
       "3    eRisk2023-T3_Subject4                   b' '    b'2022-06-28 18:54:00'    \n",
       "4    eRisk2023-T3_Subject4                   b' '    b'2022-06-28 18:31:07'    \n",
       "..                     ...                     ...                       ...   \n",
       "103  eRisk2023-T3_Subject4                   b' '    b'2019-09-20 09:20:37'    \n",
       "104  eRisk2023-T3_Subject4                   b' '    b'2019-09-19 14:53:23'    \n",
       "105  eRisk2023-T3_Subject4                   b' '    b'2019-09-03 13:54:33'    \n",
       "106  eRisk2023-T3_Subject4   b'Keeping it simple'    b'2019-09-03 13:50:30'    \n",
       "107  eRisk2023-T3_Subject4                   b' '    b'2019-08-22 11:17:49'    \n",
       "\n",
       "      WRITING.INFO                          WRITING.TEXT.Element:Text  \n",
       "0     reddit post   For the leangains method: RePT Fitness workout...  \n",
       "1     reddit post   Is there some kind of nuance that I\\'m missing...  \n",
       "2     reddit post   If you connect the app to google fit, you can ...  \n",
       "3     reddit post   It's the EufyLife app that works with the Eufy...  \n",
       "4     reddit post   As long as I've got enough black coffee to get...  \n",
       "..             ...                                                ...  \n",
       "103   reddit post   Oh, yeah, so it is. [sad emoticon] Yeah, I've ...  \n",
       "104   reddit post   Last week a group of people changed all of the...  \n",
       "105   reddit post   500 calorie lunch 200g of leftover barbecued b...  \n",
       "106   reddit post                                                 NaN  \n",
       "107   reddit post   Some people criticise the programme but I thin...  \n",
       "\n",
       "[108 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset\\eRisk2023-T3_Subject4.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop through all the data and fill the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new dataframe, with ID and text. nothing should be there in it. i will append the data to it\n",
    "df_1 = pd.DataFrame(columns=['Subject', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for i in range(1, 47):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(f'Dataset\\eRisk2023-T3_Subject{i}.csv')\n",
    "    \n",
    "    # Append the data to df_new\n",
    "    data_to_append = \"\"\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if isinstance(row['WRITING.TEXT.Element:Text'], str):\n",
    "            data_to_append = data_to_append + row['WRITING.TEXT.Element:Text'] + \" \"\n",
    "\n",
    "    final_data = {'Subject': f'eRisk2023-T3_Subject{i}', 'text': data_to_append}\n",
    "    data_list.append(final_data)\n",
    "\n",
    "df_1 = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eRisk2023-T3_Subject1</td>\n",
       "      <td>Idk mate, since 1796 we have been using vaccin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eRisk2023-T3_Subject2</td>\n",
       "      <td>I loved the idea/concept but I feel like the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eRisk2023-T3_Subject3</td>\n",
       "      <td>You don't need any certification up until you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eRisk2023-T3_Subject4</td>\n",
       "      <td>For the leangains method: RePT Fitness workout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eRisk2023-T3_Subject5</td>\n",
       "      <td>I think, in my opinion, it\\'s because the syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eRisk2023-T3_Subject6</td>\n",
       "      <td>I volunteer! No, but I waited until my late 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eRisk2023-T3_Subject7</td>\n",
       "      <td>find self thinking of food a lot, and wanting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eRisk2023-T3_Subject8</td>\n",
       "      <td>Wales is a country, but Catalonia is not. 3\\xe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>eRisk2023-T3_Subject9</td>\n",
       "      <td>Function words are everything for this task, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>eRisk2023-T3_Subject10</td>\n",
       "      <td>EM - polypharmacy. It's not so much what this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>eRisk2023-T3_Subject11</td>\n",
       "      <td>Don't execute enemies, just kill them. Other t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>eRisk2023-T3_Subject12</td>\n",
       "      <td>I'd say for me soap isn't the best description...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>eRisk2023-T3_Subject13</td>\n",
       "      <td>Done :) Done! [heart emoticon] Go conquer that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>eRisk2023-T3_Subject14</td>\n",
       "      <td>yeah, i mainly eat in the evenings or at night...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>eRisk2023-T3_Subject15</td>\n",
       "      <td>agreed lol the latter ill manifest a mermaid o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eRisk2023-T3_Subject16</td>\n",
       "      <td>Looking for one GA ticket, message me if you'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>eRisk2023-T3_Subject17</td>\n",
       "      <td>I don't experience gender envy actually. I see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>eRisk2023-T3_Subject18</td>\n",
       "      <td>Thank you! That's so nice of you to say! Wow t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>eRisk2023-T3_Subject19</td>\n",
       "      <td>Have you read the comment section? A lot of co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>eRisk2023-T3_Subject20</td>\n",
       "      <td>&gt;Also, likely relative currency valuations are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>eRisk2023-T3_Subject21</td>\n",
       "      <td>It might be a bit off track for this post, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>eRisk2023-T3_Subject22</td>\n",
       "      <td>technically yes, but not by very much so i'm n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>eRisk2023-T3_Subject23</td>\n",
       "      <td>geoguessr.com In that case you guys win second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>eRisk2023-T3_Subject24</td>\n",
       "      <td>I use alcohol with it because I learnt somewhe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>eRisk2023-T3_Subject25</td>\n",
       "      <td>Kindly from the rest of the first world: the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>eRisk2023-T3_Subject26</td>\n",
       "      <td>Anti matter does not have negative mass nor ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>eRisk2023-T3_Subject27</td>\n",
       "      <td>I think a few things would be fitting:  A preq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>eRisk2023-T3_Subject28</td>\n",
       "      <td>I get it. I never lost weight but I always had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>eRisk2023-T3_Subject29</td>\n",
       "      <td>I think in my personal experience, anxiety cau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>eRisk2023-T3_Subject30</td>\n",
       "      <td>Jesse/Festus Love it. My new absolute favourit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>eRisk2023-T3_Subject31</td>\n",
       "      <td>Yeah Ok thanks   It's not the it's still dark ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>eRisk2023-T3_Subject32</td>\n",
       "      <td>Listen, you can feel however you want, but a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>eRisk2023-T3_Subject33</td>\n",
       "      <td>cramming tooo!! add me to any study gc so we c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>eRisk2023-T3_Subject34</td>\n",
       "      <td>You have a beautiful hair ma'am but I was actu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>eRisk2023-T3_Subject35</td>\n",
       "      <td>I think that if he's curious about what you're...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>eRisk2023-T3_Subject36</td>\n",
       "      <td>i don't get what the caption is trying to say?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>eRisk2023-T3_Subject37</td>\n",
       "      <td>It's a moldy geode Hehe, great minds think ali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>eRisk2023-T3_Subject38</td>\n",
       "      <td>Very cool looking game so far I'll be sure to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>eRisk2023-T3_Subject39</td>\n",
       "      <td>Hey thanks for ur answer and the resources tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>eRisk2023-T3_Subject40</td>\n",
       "      <td>Smile. I love when people smile, relieves A LO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>eRisk2023-T3_Subject41</td>\n",
       "      <td>&gt;I\\'d recommend talking to a parent  They don\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>eRisk2023-T3_Subject42</td>\n",
       "      <td>Is that a joke? He's commercialised a huge num...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>eRisk2023-T3_Subject43</td>\n",
       "      <td>i am also infertile and it's nothing to be jea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>eRisk2023-T3_Subject44</td>\n",
       "      <td>Then I have no idea Siege is ok but overwatch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>eRisk2023-T3_Subject45</td>\n",
       "      <td>Salsa and pickles Enchilada, Popsicle, Hummus,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>eRisk2023-T3_Subject46</td>\n",
       "      <td>I have this one too! Mainly comes out when I'm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Subject                                               text\n",
       "0    eRisk2023-T3_Subject1  Idk mate, since 1796 we have been using vaccin...\n",
       "1    eRisk2023-T3_Subject2  I loved the idea/concept but I feel like the f...\n",
       "2    eRisk2023-T3_Subject3  You don't need any certification up until you ...\n",
       "3    eRisk2023-T3_Subject4  For the leangains method: RePT Fitness workout...\n",
       "4    eRisk2023-T3_Subject5  I think, in my opinion, it\\'s because the syst...\n",
       "5    eRisk2023-T3_Subject6  I volunteer! No, but I waited until my late 30...\n",
       "6    eRisk2023-T3_Subject7  find self thinking of food a lot, and wanting ...\n",
       "7    eRisk2023-T3_Subject8  Wales is a country, but Catalonia is not. 3\\xe...\n",
       "8    eRisk2023-T3_Subject9  Function words are everything for this task, n...\n",
       "9   eRisk2023-T3_Subject10  EM - polypharmacy. It's not so much what this ...\n",
       "10  eRisk2023-T3_Subject11  Don't execute enemies, just kill them. Other t...\n",
       "11  eRisk2023-T3_Subject12  I'd say for me soap isn't the best description...\n",
       "12  eRisk2023-T3_Subject13  Done :) Done! [heart emoticon] Go conquer that...\n",
       "13  eRisk2023-T3_Subject14  yeah, i mainly eat in the evenings or at night...\n",
       "14  eRisk2023-T3_Subject15  agreed lol the latter ill manifest a mermaid o...\n",
       "15  eRisk2023-T3_Subject16  Looking for one GA ticket, message me if you'r...\n",
       "16  eRisk2023-T3_Subject17  I don't experience gender envy actually. I see...\n",
       "17  eRisk2023-T3_Subject18  Thank you! That's so nice of you to say! Wow t...\n",
       "18  eRisk2023-T3_Subject19  Have you read the comment section? A lot of co...\n",
       "19  eRisk2023-T3_Subject20  >Also, likely relative currency valuations are...\n",
       "20  eRisk2023-T3_Subject21  It might be a bit off track for this post, but...\n",
       "21  eRisk2023-T3_Subject22  technically yes, but not by very much so i'm n...\n",
       "22  eRisk2023-T3_Subject23  geoguessr.com In that case you guys win second...\n",
       "23  eRisk2023-T3_Subject24  I use alcohol with it because I learnt somewhe...\n",
       "24  eRisk2023-T3_Subject25  Kindly from the rest of the first world: the r...\n",
       "25  eRisk2023-T3_Subject26  Anti matter does not have negative mass nor ne...\n",
       "26  eRisk2023-T3_Subject27  I think a few things would be fitting:  A preq...\n",
       "27  eRisk2023-T3_Subject28  I get it. I never lost weight but I always had...\n",
       "28  eRisk2023-T3_Subject29  I think in my personal experience, anxiety cau...\n",
       "29  eRisk2023-T3_Subject30  Jesse/Festus Love it. My new absolute favourit...\n",
       "30  eRisk2023-T3_Subject31  Yeah Ok thanks   It's not the it's still dark ...\n",
       "31  eRisk2023-T3_Subject32  Listen, you can feel however you want, but a r...\n",
       "32  eRisk2023-T3_Subject33  cramming tooo!! add me to any study gc so we c...\n",
       "33  eRisk2023-T3_Subject34  You have a beautiful hair ma'am but I was actu...\n",
       "34  eRisk2023-T3_Subject35  I think that if he's curious about what you're...\n",
       "35  eRisk2023-T3_Subject36  i don't get what the caption is trying to say?...\n",
       "36  eRisk2023-T3_Subject37  It's a moldy geode Hehe, great minds think ali...\n",
       "37  eRisk2023-T3_Subject38  Very cool looking game so far I'll be sure to ...\n",
       "38  eRisk2023-T3_Subject39  Hey thanks for ur answer and the resources tha...\n",
       "39  eRisk2023-T3_Subject40  Smile. I love when people smile, relieves A LO...\n",
       "40  eRisk2023-T3_Subject41  >I\\'d recommend talking to a parent  They don\\...\n",
       "41  eRisk2023-T3_Subject42  Is that a joke? He's commercialised a huge num...\n",
       "42  eRisk2023-T3_Subject43  i am also infertile and it's nothing to be jea...\n",
       "43  eRisk2023-T3_Subject44  Then I have no idea Siege is ok but overwatch ...\n",
       "44  eRisk2023-T3_Subject45  Salsa and pickles Enchilada, Popsicle, Hummus,...\n",
       "45  eRisk2023-T3_Subject46  I have this one too! Mainly comes out when I'm..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Preprocessing for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_1.iterrows():\n",
    "    text_value = row['text']\n",
    "\n",
    "    # lowercasing\n",
    "    text_value = text_value.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text_value = remove_urls(text_value)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text_value = re.sub(r'[^\\w\\s]', '', text_value)\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_value = ' '.join([word for word in text_value.split() if word not in stop_words])\n",
    "\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text_value = ' '.join([lemmatizer.lemmatize(word) for word in text_value.split()])\n",
    "    \n",
    "    # store the cleaned text\n",
    "    df_1.at[index, 'text'] = text_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eRisk2023-T3_Subject1</td>\n",
       "      <td>idk mate since 1796 using vaccine still waitin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eRisk2023-T3_Subject2</td>\n",
       "      <td>loved ideaconcept feel like film left many thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eRisk2023-T3_Subject3</td>\n",
       "      <td>dont need certification start working h motor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eRisk2023-T3_Subject4</td>\n",
       "      <td>leangains method rept fitness workout tracker ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eRisk2023-T3_Subject5</td>\n",
       "      <td>think opinion system led anxiety havent change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eRisk2023-T3_Subject6</td>\n",
       "      <td>volunteer waited late 30 family took much ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eRisk2023-T3_Subject7</td>\n",
       "      <td>find self thinking food lot wanting eat etc ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eRisk2023-T3_Subject8</td>\n",
       "      <td>wale country catalonia 3xe2x98xbaxefxb8x8f def...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>eRisk2023-T3_Subject9</td>\n",
       "      <td>function word everything task content word muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>eRisk2023-T3_Subject10</td>\n",
       "      <td>em polypharmacy much 80yo gramma taking idea s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>eRisk2023-T3_Subject11</td>\n",
       "      <td>dont execute enemy kill didnt like hk first fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>eRisk2023-T3_Subject12</td>\n",
       "      <td>id say soap isnt best description taste like t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>eRisk2023-T3_Subject13</td>\n",
       "      <td>done done heart emoticon go conquer ca creativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>eRisk2023-T3_Subject14</td>\n",
       "      <td>yeah mainly eat evening night dont eat bed get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>eRisk2023-T3_Subject15</td>\n",
       "      <td>agreed lol latter ill manifest mermaid orgy fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eRisk2023-T3_Subject16</td>\n",
       "      <td>looking one ga ticket message youre trying get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>eRisk2023-T3_Subject17</td>\n",
       "      <td>dont experience gender envy actually see gende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>eRisk2023-T3_Subject18</td>\n",
       "      <td>thank thats nice say wow thanks awww thank muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>eRisk2023-T3_Subject19</td>\n",
       "      <td>read comment section lot comment posted autist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>eRisk2023-T3_Subject20</td>\n",
       "      <td>also likely relative currency valuation import...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>eRisk2023-T3_Subject21</td>\n",
       "      <td>might bit track post someone explain see much ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>eRisk2023-T3_Subject22</td>\n",
       "      <td>technically yes much im sure would make much d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>eRisk2023-T3_Subject23</td>\n",
       "      <td>geoguessrcom case guy win second racist three ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>eRisk2023-T3_Subject24</td>\n",
       "      <td>use alcohol learnt somewhere alcohol would enh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>eRisk2023-T3_Subject25</td>\n",
       "      <td>kindly rest first world rest first world gun p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>eRisk2023-T3_Subject26</td>\n",
       "      <td>anti matter negative mass negative energy anti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>eRisk2023-T3_Subject27</td>\n",
       "      <td>think thing would fitting prequel gertrude ger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>eRisk2023-T3_Subject28</td>\n",
       "      <td>get never lost weight always feeling empty sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>eRisk2023-T3_Subject29</td>\n",
       "      <td>think personal experience anxiety caused lot c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>eRisk2023-T3_Subject30</td>\n",
       "      <td>jessefestus love new absolute favourite fragra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>eRisk2023-T3_Subject31</td>\n",
       "      <td>yeah ok thanks still dark give one worst as va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>eRisk2023-T3_Subject32</td>\n",
       "      <td>listen feel however want random post russian f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>eRisk2023-T3_Subject33</td>\n",
       "      <td>cramming tooo add study gc cram together count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>eRisk2023-T3_Subject34</td>\n",
       "      <td>beautiful hair maam actually impressed beauty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>eRisk2023-T3_Subject35</td>\n",
       "      <td>think he curious youre phone show bit trust do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>eRisk2023-T3_Subject36</td>\n",
       "      <td>dont get caption trying say power move ixcaxb5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>eRisk2023-T3_Subject37</td>\n",
       "      <td>moldy geode hehe great mind think alike go fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>eRisk2023-T3_Subject38</td>\n",
       "      <td>cool looking game far ill sure join community ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>eRisk2023-T3_Subject39</td>\n",
       "      <td>hey thanks ur answer resource thats really hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>eRisk2023-T3_Subject40</td>\n",
       "      <td>smile love people smile relief lot pressure pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>eRisk2023-T3_Subject41</td>\n",
       "      <td>id recommend talking parent dont care father s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>eRisk2023-T3_Subject42</td>\n",
       "      <td>joke he commercialised huge number product man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>eRisk2023-T3_Subject43</td>\n",
       "      <td>also infertile nothing jealous like understand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>eRisk2023-T3_Subject44</td>\n",
       "      <td>idea siege ok overwatch suck go play tf2 inste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>eRisk2023-T3_Subject45</td>\n",
       "      <td>salsa pickle enchilada popsicle hummus chalupa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>eRisk2023-T3_Subject46</td>\n",
       "      <td>one mainly come im social setting ive tried lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Subject                                               text\n",
       "0    eRisk2023-T3_Subject1  idk mate since 1796 using vaccine still waitin...\n",
       "1    eRisk2023-T3_Subject2  loved ideaconcept feel like film left many thi...\n",
       "2    eRisk2023-T3_Subject3  dont need certification start working h motor ...\n",
       "3    eRisk2023-T3_Subject4  leangains method rept fitness workout tracker ...\n",
       "4    eRisk2023-T3_Subject5  think opinion system led anxiety havent change...\n",
       "5    eRisk2023-T3_Subject6  volunteer waited late 30 family took much ther...\n",
       "6    eRisk2023-T3_Subject7  find self thinking food lot wanting eat etc ac...\n",
       "7    eRisk2023-T3_Subject8  wale country catalonia 3xe2x98xbaxefxb8x8f def...\n",
       "8    eRisk2023-T3_Subject9  function word everything task content word muc...\n",
       "9   eRisk2023-T3_Subject10  em polypharmacy much 80yo gramma taking idea s...\n",
       "10  eRisk2023-T3_Subject11  dont execute enemy kill didnt like hk first fe...\n",
       "11  eRisk2023-T3_Subject12  id say soap isnt best description taste like t...\n",
       "12  eRisk2023-T3_Subject13  done done heart emoticon go conquer ca creativ...\n",
       "13  eRisk2023-T3_Subject14  yeah mainly eat evening night dont eat bed get...\n",
       "14  eRisk2023-T3_Subject15  agreed lol latter ill manifest mermaid orgy fr...\n",
       "15  eRisk2023-T3_Subject16  looking one ga ticket message youre trying get...\n",
       "16  eRisk2023-T3_Subject17  dont experience gender envy actually see gende...\n",
       "17  eRisk2023-T3_Subject18  thank thats nice say wow thanks awww thank muc...\n",
       "18  eRisk2023-T3_Subject19  read comment section lot comment posted autist...\n",
       "19  eRisk2023-T3_Subject20  also likely relative currency valuation import...\n",
       "20  eRisk2023-T3_Subject21  might bit track post someone explain see much ...\n",
       "21  eRisk2023-T3_Subject22  technically yes much im sure would make much d...\n",
       "22  eRisk2023-T3_Subject23  geoguessrcom case guy win second racist three ...\n",
       "23  eRisk2023-T3_Subject24  use alcohol learnt somewhere alcohol would enh...\n",
       "24  eRisk2023-T3_Subject25  kindly rest first world rest first world gun p...\n",
       "25  eRisk2023-T3_Subject26  anti matter negative mass negative energy anti...\n",
       "26  eRisk2023-T3_Subject27  think thing would fitting prequel gertrude ger...\n",
       "27  eRisk2023-T3_Subject28  get never lost weight always feeling empty sto...\n",
       "28  eRisk2023-T3_Subject29  think personal experience anxiety caused lot c...\n",
       "29  eRisk2023-T3_Subject30  jessefestus love new absolute favourite fragra...\n",
       "30  eRisk2023-T3_Subject31  yeah ok thanks still dark give one worst as va...\n",
       "31  eRisk2023-T3_Subject32  listen feel however want random post russian f...\n",
       "32  eRisk2023-T3_Subject33  cramming tooo add study gc cram together count...\n",
       "33  eRisk2023-T3_Subject34  beautiful hair maam actually impressed beauty ...\n",
       "34  eRisk2023-T3_Subject35  think he curious youre phone show bit trust do...\n",
       "35  eRisk2023-T3_Subject36  dont get caption trying say power move ixcaxb5...\n",
       "36  eRisk2023-T3_Subject37  moldy geode hehe great mind think alike go fin...\n",
       "37  eRisk2023-T3_Subject38  cool looking game far ill sure join community ...\n",
       "38  eRisk2023-T3_Subject39  hey thanks ur answer resource thats really hel...\n",
       "39  eRisk2023-T3_Subject40  smile love people smile relief lot pressure pe...\n",
       "40  eRisk2023-T3_Subject41  id recommend talking parent dont care father s...\n",
       "41  eRisk2023-T3_Subject42  joke he commercialised huge number product man...\n",
       "42  eRisk2023-T3_Subject43  also infertile nothing jealous like understand...\n",
       "43  eRisk2023-T3_Subject44  idea siege ok overwatch suck go play tf2 inste...\n",
       "44  eRisk2023-T3_Subject45  salsa pickle enchilada popsicle hummus chalupa...\n",
       "45  eRisk2023-T3_Subject46  one mainly come im social setting ive tried lo..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_pred = df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 0, 1, 2, 6, 6, 0, 5, 6, 4, 2, 0, 5, 6, 6, 6, 5, 0, 1, 6, 6, 3, 6, 3, 6, 6, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# open the text file and read it\n",
    "labels_1_test = []\n",
    "with open('ground-truth_eRisk2022_T3.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # go till the first space or tab is found\n",
    "        index = 0\n",
    "        while line[index] != ' ' and line[index] != '\\t':\n",
    "            index += 1\n",
    "\n",
    "        # go till the first digit character is found\n",
    "        while not line[index].isdigit():\n",
    "            index += 1\n",
    "        \n",
    "        # get the label\n",
    "        labels_1_test.append(line[index])\n",
    "\n",
    "# convert the labels_1 into digits\n",
    "for i in range(len(labels_1_test)):\n",
    "    if labels_1_test[i] == '0':\n",
    "        labels_1_test[i] = 0\n",
    "    elif labels_1_test[i] == '1':\n",
    "        labels_1_test[i] = 1\n",
    "    elif labels_1_test[i] == '2':\n",
    "        labels_1_test[i] = 2\n",
    "    elif labels_1_test[i] == '3':\n",
    "        labels_1_test[i] = 3\n",
    "    elif labels_1_test[i] == '4':\n",
    "        labels_1_test[i] = 4\n",
    "    elif labels_1_test[i] == '5':\n",
    "        labels_1_test[i] = 5\n",
    "    else:\n",
    "        labels_1_test[i] = 6 \n",
    "    \n",
    "\n",
    "print(labels_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (28) does not match length of index (46)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_1_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m labels_1_test\n\u001b[0;32m      2\u001b[0m df_1_pred\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\91900\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3978\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3977\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3978\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91900\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4172\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4163\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4164\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4165\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4170\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4171\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4172\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4175\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4176\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4177\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   4178\u001b[0m     ):\n\u001b[0;32m   4179\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4180\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\91900\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4912\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4909\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4912\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\91900\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (28) does not match length of index (46)"
     ]
    }
   ],
   "source": [
    "df_1_pred['label'] = labels_1_test\n",
    "df_1_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "\n",
    "X_train = df_1_pred['text']\n",
    "y_train = df_1_pred['target']\n",
    "\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'WRITING.TEXT' column found in subject_1.csv\n",
      "No 'WRITING.TEXT' column found in subject_2.csv\n",
      "Renamed 'WRITING.TEXT' to 'WRITING.TEXT.Element:Text' in subject_3.csv\n",
      "No 'WRITING.TEXT' column found in subject_4.csv\n",
      "No 'WRITING.TEXT' column found in subject_5.csv\n",
      "No 'WRITING.TEXT' column found in subject_6.csv\n",
      "No 'WRITING.TEXT' column found in subject_7.csv\n",
      "No 'WRITING.TEXT' column found in subject_8.csv\n",
      "No 'WRITING.TEXT' column found in subject_9.csv\n",
      "No 'WRITING.TEXT' column found in subject_10.csv\n",
      "No 'WRITING.TEXT' column found in subject_11.csv\n",
      "No 'WRITING.TEXT' column found in subject_12.csv\n",
      "No 'WRITING.TEXT' column found in subject_13.csv\n",
      "No 'WRITING.TEXT' column found in subject_14.csv\n",
      "No 'WRITING.TEXT' column found in subject_15.csv\n",
      "Renamed 'WRITING.TEXT' to 'WRITING.TEXT.Element:Text' in subject_16.csv\n",
      "No 'WRITING.TEXT' column found in subject_17.csv\n",
      "Renamed 'WRITING.TEXT' to 'WRITING.TEXT.Element:Text' in subject_18.csv\n",
      "No 'WRITING.TEXT' column found in subject_19.csv\n",
      "Renamed 'WRITING.TEXT' to 'WRITING.TEXT.Element:Text' in subject_20.csv\n",
      "No 'WRITING.TEXT' column found in subject_21.csv\n",
      "No 'WRITING.TEXT' column found in subject_22.csv\n",
      "No 'WRITING.TEXT' column found in subject_23.csv\n",
      "No 'WRITING.TEXT' column found in subject_24.csv\n",
      "No 'WRITING.TEXT' column found in subject_25.csv\n",
      "No 'WRITING.TEXT' column found in subject_26.csv\n",
      "No 'WRITING.TEXT' column found in subject_27.csv\n",
      "No 'WRITING.TEXT' column found in subject_28.csv\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 29):\n",
    "    df = pd.read_csv(f'Test\\eRisk2022-T3_Subject{i}.csv')\n",
    "    \n",
    "    # If a column is called \"WRITING.TEXT\", rename it to \"WRITING.TEXT.Element:Text\"\n",
    "    if \"WRITING.TEXT\" in df.columns:\n",
    "        df.rename(columns={\"WRITING.TEXT\": \"WRITING.TEXT.Element:Text\"}, inplace=True)\n",
    "        df.to_csv(f'Test\\eRisk2022-T3_Subject{i}.csv', index=False)  # Save the modified DataFrame\n",
    "        print(f\"Renamed 'WRITING.TEXT' to 'WRITING.TEXT.Element:Text' in subject_{i}.csv\")\n",
    "    else:\n",
    "        print(f\"No 'WRITING.TEXT' column found in subject_{i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 29):\n",
    "\n",
    "    df = pd.read_csv(f'Test\\eRisk2022-T3_Subject{i}.csv')\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        text_value = row['WRITING.TEXT.Element:Text']\n",
    "\n",
    "        if isinstance(text_value, str):\n",
    "\n",
    "            if ((text_value[2] == \"'\" or text_value[2] == '\"') and (text_value[-2] == \"'\" or text_value[-2] == '\"') and (text_value[0] == ' ' and text_value[1] == 'b' and text_value[-1] == ' ')):\n",
    "                df.at[index, 'WRITING.TEXT.Element:Text'] = text_value[3:-2]\n",
    "                text_value = text_value[3:-2]\n",
    "\n",
    "            # Replace specific substrings\n",
    "            text_value = text_value.replace('\\\\xe2\\\\x80\\\\x99', \"'\")\n",
    "            text_value = text_value.replace('\\\\xe2\\\\x80\\\\x94', \"-\")\n",
    "            text_value = text_value.replace('\\\\n', \" \")\n",
    "\n",
    "            # Define the regular expression pattern to match words starting with \\xf0\\x9f\n",
    "            pattern = r'\\\\xf0\\\\x9f\\S*'\n",
    "            # Replace matched words with a single space\n",
    "            text_value = re.sub(pattern, ' ', text_value)\n",
    "\n",
    "            # Replace emoji with their text description\n",
    "            text_value = emoji.demojize(text_value)\n",
    "            df.at[index, 'WRITING.TEXT.Element:Text'] = text_value\n",
    "            \n",
    "        elif isinstance(text_value, float) and np.isnan(text_value):\n",
    "            # Handle NaN values if needed\n",
    "            pass\n",
    "\n",
    "    df.to_csv(f'Test\\eRisk2022-T3_Subject{i}.csv', index=False)  # Save the modified DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_test = pd.DataFrame(columns=['Subject', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for i in range(1, 29):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(f'Test\\eRisk2022-T3_Subject{i}.csv')\n",
    "    \n",
    "    # Append the data to df_new\n",
    "    data_to_append = \"\"\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if isinstance(row['WRITING.TEXT.Element:Text'], str):\n",
    "            data_to_append = data_to_append + row['WRITING.TEXT.Element:Text'] + \" \"\n",
    "\n",
    "    final_data = {'Subject': f'eRisk2022-T3_Subject{i}', 'text': data_to_append}\n",
    "    data_list.append(final_data)\n",
    "\n",
    "df_1_test = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_1_test.iterrows():\n",
    "    text_value = row['text']\n",
    "\n",
    "    # lowercasing\n",
    "    text_value = text_value.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text_value = remove_urls(text_value)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text_value = re.sub(r'[^\\w\\s]', '', text_value)\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_value = ' '.join([word for word in text_value.split() if word not in stop_words])\n",
    "\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text_value = ' '.join([lemmatizer.lemmatize(word) for word in text_value.split()])\n",
    "    \n",
    "    # store the cleaned text\n",
    "    df_1_test.at[index, 'text'] = text_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eRisk2022-T3_Subject1</td>\n",
       "      <td>thats thats post fucking utterly completely lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eRisk2022-T3_Subject2</td>\n",
       "      <td>shes wrong 25 warm rtightpussy thats spicy gir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eRisk2022-T3_Subject3</td>\n",
       "      <td>feel way ill sometimes starve im feeling stres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eRisk2022-T3_Subject4</td>\n",
       "      <td>yoga vvvv helpful slowly concentrate movement ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eRisk2022-T3_Subject5</td>\n",
       "      <td>im experience ive owned patent leather suede b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Subject                                               text\n",
       "0  eRisk2022-T3_Subject1  thats thats post fucking utterly completely lo...\n",
       "1  eRisk2022-T3_Subject2  shes wrong 25 warm rtightpussy thats spicy gir...\n",
       "2  eRisk2022-T3_Subject3  feel way ill sometimes starve im feeling stres...\n",
       "3  eRisk2022-T3_Subject4  yoga vvvv helpful slowly concentrate movement ...\n",
       "4  eRisk2022-T3_Subject5  im experience ive owned patent leather suede b..."
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# open the text file and read it\n",
    "labels_1_test = []\n",
    "with open('ground-truth_eRisk2022_T3.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # go till the first space or tab is found\n",
    "        index = 0\n",
    "        while line[index] != ' ' and line[index] != '\\t':\n",
    "            index += 1\n",
    "\n",
    "        # go till the first digit character is found\n",
    "        while not line[index].isdigit():\n",
    "            index += 1\n",
    "        \n",
    "        # get the label\n",
    "        labels_1_test.append(line[index])\n",
    "\n",
    "# convert the labels_1_test into digits\n",
    "for i in range(len(labels_1_test)):\n",
    "    if labels_1_test[i] == '0':\n",
    "        labels_1_test[i] = 0\n",
    "    elif labels_1_test[i] == '1':\n",
    "        labels_1_test[i] = 1\n",
    "    elif labels_1_test[i] == '2':\n",
    "        labels_1_test[i] = 2\n",
    "    elif labels_1_test[i] == '3':\n",
    "        labels_1_test[i] = 3\n",
    "    elif labels_1_test[i] == '4':\n",
    "        labels_1_test[i] = 4\n",
    "    elif labels_1_test[i] == '5':\n",
    "        labels_1_test[i] = 5\n",
    "    else:\n",
    "        labels_1_test[i] = 6\n",
    "\n",
    "print(len(labels_1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.42857142857142855\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         3\n",
      "           6       0.43      1.00      0.60        12\n",
      "\n",
      "    accuracy                           0.43        28\n",
      "   macro avg       0.06      0.14      0.09        28\n",
      "weighted avg       0.18      0.43      0.26        28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91900\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\91900\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\91900\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = nb.predict(df_1_test['text'])\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, labels_1_test))\n",
    "print(classification_report(labels_1_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
